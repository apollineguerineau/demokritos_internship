url,title,description,score,get_with_query,time_fetch,is_seed
http://arxiv.org/abs/2402.18150v2,"Unsupervised Information Refinement Training of Large Language Models
  for Retrieval-Augmented Generation","  Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating additional information from retrieval. However, studies have
shown that LLMs still face challenges in effectively using the retrieved
information, even ignoring it or being misled by it. The key reason is that the
training of LLMs does not clearly make LLMs learn how to utilize input
retrieved texts with varied quality. In this paper, we propose a novel
perspective that considers the role of LLMs in RAG as ``Information Refiner'',
which means that regardless of correctness, completeness, or usefulness of
retrieved texts, LLMs can consistently integrate knowledge within the retrieved
texts and model parameters to generate the texts that are more concise,
accurate, and complete than the retrieved texts. To this end, we propose an
information refinement training method named InFO-RAG that optimizes LLMs for
RAG in an unsupervised manner. InFO-RAG is low-cost and general across various
tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse
tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,
and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an
average of 9.39\% relative points. InFO-RAG also shows advantages in in-context
learning and robustness of RAG.
",0.8225449694467256,"RAG AND ""code generation""",2024-11-18 14:21:57.124508,False
http://arxiv.org/abs/2407.02742v1,"A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized
  Retrieval Augmentation","  Natural Language to Code Generation has made significant progress in recent
years with the advent of Large Language Models(LLMs). While generation for
general-purpose languages like C, C++, and Python has improved significantly,
LLMs struggle with custom function names in Domain Specific Languages or DSLs.
This leads to higher hallucination rates and syntax errors, specially for DSLs
having a high number of custom function names. Additionally, constant updates
to function names add to the challenge as LLMs need to stay up-to-date. In this
paper, we present optimizations for using Retrieval Augmented Generation (or
RAG) with LLMs for DSL generation along with an ablation study comparing these
strategies. We generated a train as well as test dataset with a DSL to
represent automation tasks across roughly 700 APIs in public domain. We used
the training dataset to fine-tune a Codex model for this DSL. Our results
showed that the fine-tuned model scored the best on code similarity metric.
With our RAG optimizations, we achieved parity for similarity metric. The
compilation rate, however, showed that both the models still got the syntax
wrong many times, with RAG-based method being 2 pts better. Conversely,
hallucination rate for RAG model lagged by 1 pt for API names and by 2 pts for
API parameter keys. We conclude that an optimized RAG model can match the
quality of fine-tuned models and offer advantages for new, unseen APIs.
",0.855015390479451,"RAG AND ""code generation""",2024-11-18 14:21:57.726060,False
http://arxiv.org/abs/2405.01585v1,"Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular
  RAG Applications","  In recent times Large Language Models have exhibited tremendous capabilities,
especially in the areas of mathematics, code generation and general-purpose
reasoning. However for specialized domains especially in applications that
require parsing and analyzing large chunks of numeric or tabular data even
state-of-the-art (SOTA) models struggle. In this paper, we introduce a new
approach to solving domain-specific tabular data analysis tasks by presenting a
unique RAG workflow that mitigates the scalability issues of existing tabular
LLM solutions. Specifically, we present Tabular Embedding Model (TEM), a novel
approach to fine-tune embedding models for tabular Retrieval-Augmentation
Generation (RAG) applications. Embedding models form a crucial component in the
RAG workflow and even current SOTA embedding models struggle as they are
predominantly trained on textual datasets and thus underperform in scenarios
involving complex tabular data. The evaluation results showcase that our
approach not only outperforms current SOTA embedding models in this domain but
also does so with a notably smaller and more efficient model structure.
",0.81762275761665,"RAG AND ""code generation""",2024-11-18 14:21:58.305912,False
http://arxiv.org/abs/2409.20550v1,"LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism,
  and Mitigation","  Code generation aims to automatically generate code from input requirements,
significantly enhancing development efficiency. Recent large language models
(LLMs) based approaches have shown promising results and revolutionized code
generation task. Despite the promising performance, LLMs often generate
contents with hallucinations, especially for the code generation scenario
requiring the handling of complex contextual dependencies in practical
development process. Although previous study has analyzed hallucinations in
LLM-powered code generation, the study is limited to standalone function
generation. In this paper, we conduct an empirical study to study the
phenomena, mechanism, and mitigation of LLM hallucinations within more
practical and complex development contexts in repository-level generation
scenario. First, we manually examine the code generation results from six
mainstream LLMs to establish a hallucination taxonomy of LLM-generated code.
Next, we elaborate on the phenomenon of hallucinations, analyze their
distribution across different models. We then analyze causes of hallucinations
and identify four potential factors contributing to hallucinations. Finally, we
propose an RAG-based mitigation method, which demonstrates consistent
effectiveness in all studied LLMs. The replication package including code,
data, and experimental results is available at
https://github.com/DeepSoftwareAnalytics/LLMCodingHallucination
",0.8456041985343725,"RAG AND ""code generation""",2024-11-18 14:21:58.847251,False
http://arxiv.org/abs/2406.14497v1,CodeRAG-Bench: Can Retrieval Augment Code Generation?,"  While language models (LMs) have proven remarkably adept at generating code,
many programs are challenging for LMs to generate using their parametric
knowledge alone. Providing external contexts such as library documentation can
facilitate generating accurate and functional code. Despite the success of
retrieval-augmented generation (RAG) in various text-oriented tasks, its
potential for improving code generation remains under-explored. In this work,
we conduct a systematic, large-scale analysis by asking: in what scenarios can
retrieval benefit code generation models? and what challenges remain? We first
curate a comprehensive evaluation benchmark, CodeRAG-Bench, encompassing three
categories of code generation tasks, including basic programming, open-domain,
and repository-level problems. We aggregate documents from five sources for
models to retrieve contexts: competition solutions, online tutorials, library
documentation, StackOverflow posts, and GitHub repositories. We examine
top-performing models on CodeRAG-Bench by providing contexts retrieved from one
or multiple sources. While notable gains are made in final code generation by
retrieving high-quality contexts across various settings, our analysis reveals
room for improvement -- current retrievers still struggle to fetch useful
contexts especially with limited lexical overlap, and generators fail to
improve with limited context lengths or abilities to integrate additional
contexts. We hope CodeRAG-Bench serves as an effective testbed to encourage
further development of advanced code-oriented RAG methods.
",0.8561385098349755,"RAG AND ""code generation""",2024-11-18 14:21:59.419294,False
http://arxiv.org/abs/2310.04963v3,LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation,"  Large language models (LLMs) are a new and powerful tool for a wide span of
applications involving natural language and demonstrate impressive code
generation abilities. The goal of this work is to automatically generate tests
and use these tests to validate and verify compiler implementations of a
directive-based parallel programming paradigm, OpenACC. To do so, in this
paper, we explore the capabilities of state-of-the-art LLMs, including
open-source LLMs -- Meta Codellama, Phind fine-tuned version of Codellama,
Deepseek Deepseek Coder and closed-source LLMs -- OpenAI GPT-3.5-Turbo and
GPT-4-Turbo. We further fine-tuned the open-source LLMs and GPT-3.5-Turbo using
our own testsuite dataset along with using the OpenACC specification. We also
explored these LLMs using various prompt engineering techniques that include
code template, template with retrieval-augmented generation (RAG), one-shot
example, one-shot with RAG, expressive prompt with code template and RAG. This
paper highlights our findings from over 5000 tests generated via all the above
mentioned methods. Our contributions include: (a) exploring the capabilities of
the latest and relevant LLMs for code generation, (b) investigating fine-tuning
and prompt methods, and (c) analyzing the outcome of LLMs generated tests
including manually analysis of representative set of tests. We found the LLM
Deepseek-Coder-33b-Instruct produced the most passing tests followed by
GPT-4-Turbo.
",0.831133591663004,"RAG AND ""code generation""",2024-11-18 14:22:00.035244,False
http://arxiv.org/abs/2410.18251v1,Context-Augmented Code Generation Using Programming Knowledge Graphs,"  Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly
improved code generation, but, they frequently face difficulties when dealing
with challenging and complex problems. Retrieval-Augmented Generation (RAG)
addresses this issue by retrieving and integrating external knowledge at the
inference time. However, retrieval models often fail to find most relevant
context, and generation models, with limited context capacity, can hallucinate
when given irrelevant data. We present a novel framework that leverages a
Programming Knowledge Graph (PKG) to semantically represent and retrieve code.
This approach enables fine-grained code retrieval by focusing on the most
relevant segments while reducing irrelevant context through a tree-pruning
technique. PKG is coupled with a re-ranking mechanism to reduce even more
hallucinations by selectively integrating non-RAG solutions. We propose two
retrieval approaches-block-wise and function-wise-based on the PKG, optimizing
context granularity. Evaluations on the HumanEval and MBPP benchmarks show our
method improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art
models by up to 34% on MBPP. Our contributions include PKG-based retrieval,
tree pruning to enhance retrieval precision, a re-ranking method for robust
solution selection and a Fill-in-the-Middle (FIM) enhancer module for automatic
code augmentation with relevant comments and docstrings.
",0.8371341367580932,"RAG AND ""code generation""",2024-11-18 14:22:00.616370,False
http://arxiv.org/abs/2410.20975v1,"Geo-FuB: A Method for Constructing an Operator-Function Knowledge Base
  for Geospatial Code Generation Tasks Using Large Language Models","  The rise of spatiotemporal data and the need for efficient geospatial
modeling have spurred interest in automating these tasks with large language
models (LLMs). However, general LLMs often generate errors in geospatial code
due to a lack of domain-specific knowledge on functions and operators. To
address this, a retrieval-augmented generation (RAG) approach, utilizing an
external knowledge base of geospatial functions and operators, is proposed.
This study introduces a framework to construct such a knowledge base,
leveraging geospatial script semantics. The framework includes: Function
Semantic Framework Construction (Geo-FuSE), Frequent Operator Combination
Statistics (Geo-FuST), and Semantic Mapping (Geo-FuM). Techniques like
Chain-of-Thought, TF-IDF, and the APRIORI algorithm are utilized to derive and
align geospatial functions. An example knowledge base, Geo-FuB, built from
154,075 Google Earth Engine scripts, is available on GitHub. Evaluation metrics
show a high accuracy, reaching 88.89% overall, with structural and semantic
accuracies of 92.03% and 86.79% respectively. Geo-FuB's potential to optimize
geospatial code generation through the RAG and fine-tuning paradigms is
highlighted.
",0.8190978058801451,"RAG AND ""code generation""",2024-11-18 14:22:01.221184,False
http://arxiv.org/abs/2410.15154v1,"MCCoder: Streamlining Motion Control with LLM-Assisted Code Generation
  and Rigorous Verification","  Large Language Models (LLMs) have shown considerable promise in code
generation. However, the automation sector, especially in motion control,
continues to rely heavily on manual programming due to the complexity of tasks
and critical safety considerations. In this domain, incorrect code execution
can pose risks to both machinery and personnel, necessitating specialized
expertise. To address these challenges, we introduce MCCoder, an LLM-powered
system designed to generate code that addresses complex motion control tasks,
with integrated soft-motion data verification. MCCoder enhances code generation
through multitask decomposition, hybrid retrieval-augmented generation (RAG),
and self-correction with a private motion library. Moreover, it supports data
verification by logging detailed trajectory data and providing simulations and
plots, allowing users to assess the accuracy of the generated code and
bolstering confidence in LLM-based programming. To ensure robust validation, we
propose MCEVAL, an evaluation dataset with metrics tailored to motion control
tasks of varying difficulties. Experiments indicate that MCCoder improves
performance by 11.61% overall and by 66.12% on complex tasks in MCEVAL dataset
compared with base models with naive RAG. This system and dataset aim to
facilitate the application of code generation in automation settings with
strict safety requirements. MCCoder is publicly available at
https://github.com/MCCodeAI/MCCoder.
",0.8217416251402754,"RAG AND ""code generation""",2024-11-18 14:22:01.825330,False
http://arxiv.org/abs/2402.12317v1,ARKS: Active Retrieval in Knowledge Soup for Code Generation,"  Recently the retrieval-augmented generation (RAG) paradigm has raised much
attention for its potential in incorporating external knowledge into large
language models (LLMs) without further training. While widely explored in
natural language applications, its utilization in code generation remains
under-explored. In this paper, we introduce Active Retrieval in Knowledge Soup
(ARKS), an advanced strategy for generalizing large language models for code.
In contrast to relying on a single source, we construct a knowledge soup
integrating web search, documentation, execution feedback, and evolved code
snippets. We employ an active retrieval strategy that iteratively refines the
query and updates the knowledge soup. To assess the performance of ARKS, we
compile a new benchmark comprising realistic coding problems associated with
frequently updated libraries and long-tail programming languages. Experimental
results on ChatGPT and CodeLlama demonstrate a substantial improvement in the
average execution accuracy of ARKS on LLMs. The analysis confirms the
effectiveness of our proposed knowledge soup and active retrieval strategies,
offering rich insights into the construction of effective retrieval-augmented
code generation (RACG) pipelines. Our model, code, and data are available at
https://arks-codegen.github.io.
",0.8403520871779707,"RAG AND ""code generation""",2024-11-18 14:22:02.340668,False
http://arxiv.org/abs/2408.04125v1,Exploring RAG-based Vulnerability Augmentation with LLMs,"  Detecting vulnerabilities is a crucial task for maintaining the integrity,
availability, and security of software systems. Utilizing DL-based models for
vulnerability detection has become commonplace in recent years. However, such
deep learning-based vulnerability detectors (DLVD) suffer from a shortage of
sizable datasets to train effectively. Data augmentation can potentially
alleviate the shortage of data, but augmenting vulnerable code is challenging
and requires designing a generative solution that maintains vulnerability.
Hence, the work on generating vulnerable code samples has been limited and
previous works have only focused on generating samples that contain single
statements or specific types of vulnerabilities. Lately, large language models
(LLMs) are being used for solving various code generation and comprehension
tasks and have shown inspiring results, especially when fused with retrieval
augmented generation (RAG). In this study, we explore three different
strategies to augment vulnerabilities both single and multi-statement
vulnerabilities, with LLMs, namely Mutation, Injection, and Extension. We
conducted an extensive evaluation of our proposed approach on three
vulnerability datasets and three DLVD models, using two LLMs. Our results show
that our injection-based clustering-enhanced RAG method beats the baseline
setting (NoAug), Vulgen, and VGX (two SOTA methods), and Random Oversampling
(ROS) by 30.80\%, 27.48\%, 27.93\%, and 15.41\% in f1-score with 5K generated
vulnerable samples on average, and 53.84\%, 54.10\%, 69.90\%, and 40.93\% with
15K generated vulnerable samples. Our approach demonstrates its feasibility for
large-scale data augmentation by generating 1K samples at as cheap as US$ 1.88.
",0.8339607699501538,"RAG AND ""code generation""",2024-11-18 14:22:02.931879,False
http://arxiv.org/abs/2407.18333v1,"AutoVCoder: A Systematic Framework for Automated Verilog Code Generation
  using LLMs","  Recently, the use of large language models (LLMs) for software code
generation, e.g., C/C++ and Python, has proven a great success. However, LLMs
still suffer from low syntactic and functional correctness when it comes to the
generation of register-transfer level (RTL) code, such as Verilog. To address
this issue, in this paper, we develop AutoVCoder, a systematic open-source
framework that significantly improves the LLMs' correctness of generating
Verilog code and enhances the quality of its output at the same time. Our
framework integrates three novel techniques, including a high-quality hardware
dataset generation approach, a two-round LLM fine-tuning method and a
domain-specific retrieval-augmented generation (RAG) mechanism. Experimental
results demonstrate that AutoVCoder outperforms both industrial and academic
LLMs in Verilog code generation. Specifically, AutoVCoder shows a 0.5% and 2.2%
improvement in functional correctness on the EvalMachine and EvalHuman
benchmarks compared with BetterV, and also achieves a 3.4% increase in syntax
correctness and a 3.4% increase in functional correctness on the RTLLM
benchmark compared with RTLCoder.
",0.8253707216305071,"RAG AND ""code generation""",2024-11-18 14:22:03.442697,False
http://arxiv.org/abs/2410.14209v1,"Agents4PLC: Automating Closed-loop PLC Code Generation and Verification
  in Industrial Control Systems using LLM-based Agents","  In industrial control systems, the generation and verification of
Programmable Logic Controller (PLC) code are critical for ensuring operational
efficiency and safety. While Large Language Models (LLMs) have made strides in
automated code generation, they often fall short in providing correctness
guarantees and specialized support for PLC programming. To address these
challenges, this paper introduces Agents4PLC, a novel framework that not only
automates PLC code generation but also includes code-level verification through
an LLM-based multi-agent system. We first establish a comprehensive benchmark
for verifiable PLC code generation area, transitioning from natural language
requirements to human-written-verified formal specifications and reference PLC
code. We further enhance our `agents' specifically for industrial control
systems by incorporating Retrieval-Augmented Generation (RAG), advanced prompt
engineering techniques, and Chain-of-Thought strategies. Evaluation against the
benchmark demonstrates that Agents4PLC significantly outperforms previous
methods, achieving superior results across a series of increasingly rigorous
metrics. This research not only addresses the critical challenges in PLC
programming but also highlights the potential of our framework to generate
verifiable code applicable to real-world industrial applications.
",0.8029834419799338,"RAG AND ""code generation""",2024-11-18 14:22:04.013733,False
http://arxiv.org/abs/2311.16267v2,"Novel Preprocessing Technique for Data Embedding in Engineering Code
  Generation Using Large Language Model","  We present four main contributions to enhance the performance of Large
Language Models (LLMs) in generating domain-specific code: (i) utilizing
LLM-based data splitting and data renovation techniques to improve the semantic
representation of embeddings' space; (ii) introducing the Chain of Density for
Renovation Credibility (CoDRC), driven by LLMs, and the Adaptive Text
Renovation (ATR) algorithm for assessing data renovation reliability; (iii)
developing the Implicit Knowledge Expansion and Contemplation (IKEC) Prompt
technique; and (iv) effectively refactoring existing scripts to generate new
and high-quality scripts with LLMs. By using engineering simulation software
RedHawk-SC as a case study, we demonstrate the effectiveness of our data
pre-processing method for expanding and categorizing scripts. When combined
with IKEC, these techniques enhance the Retrieval-Augmented Generation (RAG)
method in retrieving more relevant information, ultimately achieving a 73.33%
""Percentage of Correct Lines"" for code generation problems in MapReduce
applications.
",0.8363659960870639,"RAG AND ""code generation""",2024-11-18 14:22:04.531276,False
http://arxiv.org/abs/2407.06245v2,"ORAN-Bench-13K: An Open Source Benchmark for Assessing LLMs in Open
  Radio Access Networks","  Large Language Models (LLMs) can revolutionize how we deploy and operate Open
Radio Access Networks (O-RAN) by enhancing network analytics, anomaly
detection, and code generation and significantly increasing the efficiency and
reliability of a plethora of O-RAN tasks. In this paper, we present
ORAN-Bench-13K, the first comprehensive benchmark designed to evaluate the
performance of Large Language Models (LLMs) within the context of O-RAN. Our
benchmark consists of 13,952 meticulously curated multiple-choice questions
generated from 116 O-RAN specification documents. We leverage a novel
three-stage LLM framework, and the questions are categorized into three
distinct difficulties to cover a wide spectrum of ORAN-related knowledge. We
thoroughly evaluate the performance of several state-of-the-art LLMs, including
Gemini, Chat-GPT, and Mistral. Additionally, we propose ORANSight, a
Retrieval-Augmented Generation (RAG)-based pipeline that demonstrates superior
performance on ORAN-Bench-13K compared to other tested closed-source models.
Our findings indicate that current popular LLM models are not proficient in
O-RAN, highlighting the need for specialized models. We observed a noticeable
performance improvement when incorporating the RAG-based ORANSight pipeline,
with a Macro Accuracy of 0.784 and a Weighted Accuracy of 0.776, which was on
average 21.55% and 22.59% better than the other tested LLMs.
",0.8087237598031558,"RAG AND ""code generation""",2024-11-18 14:22:05.229835,False
http://arxiv.org/abs/2408.08335v1,Plan with Code: Comparing approaches for robust NL to DSL generation,"  Planning in code is considered a more reliable approach for many
orchestration tasks. This is because code is more tractable than steps
generated via Natural Language and make it easy to support more complex
sequences by abstracting deterministic logic into functions. It also allows
spotting issues with incorrect function names with the help of parsing checks
that can be run on code. Progress in Code Generation methodologies, however,
remains limited to general-purpose languages like C, C++, and Python. LLMs
continue to face challenges with custom function names in Domain Specific
Languages or DSLs, leading to higher hallucination rates and syntax errors.
This is more common for custom function names, that are typically part of the
plan. Moreover, keeping LLMs up-to-date with newer function names is an issue.
This poses a challenge for scenarios like task planning over a large number of
APIs, since the plan is represented as a DSL having custom API names. In this
paper, we focus on workflow automation in RPA (Robotic Process Automation)
domain as a special case of task planning. We present optimizations for using
Retrieval Augmented Generation (or RAG) with LLMs for DSL generation along with
an ablation study comparing these strategies with a fine-tuned model. Our
results showed that the fine-tuned model scored the best on code similarity
metric. However, with our optimizations, RAG approach is able to match the
quality for in-domain API names in the test set. Additionally, it offers
significant advantage for out-of-domain or unseen API names, outperforming
Fine-Tuned model on similarity metric by 7 pts.
",0.8464763838948021,"RAG AND ""code generation""",2024-11-18 14:22:05.750964,False
http://arxiv.org/abs/2409.15204v2,RAMBO: Enhancing RAG-based Repository-Level Method Body Completion,"  Code completion is essential in software development, helping developers by
predicting code snippets based on context. Among completion tasks, Method Body
Completion (MBC) is particularly challenging as it involves generating complete
method bodies based on their signatures and context. This task becomes
significantly harder in large repositories, where method bodies must integrate
repositoryspecific elements such as custom APIs, inter-module dependencies, and
project-specific conventions. In this paper, we introduce RAMBO, a novel
RAG-based approach for repository-level MBC. Instead of retrieving similar
method bodies, RAMBO identifies essential repository-specific elements, such as
classes, methods, and variables/fields, and their relevant usages. By
incorporating these elements and their relevant usages into the code generation
process, RAMBO ensures more accurate and contextually relevant method bodies.
Our experimental results with leading code LLMs across 40 Java projects show
that RAMBO significantly outperformed the state-of-the-art repository-level MBC
approaches, with the improvements of up to 46% in BLEU, 57% in CodeBLEU, 36% in
Compilation Rate, and up to 3X in Exact Match. Notably, RAMBO surpassed
RepoCoder Oracle method by up to 12% in Exact Match, setting a new benchmark
for repository-level MBC.
",0.8283627318926843,"RAG AND ""code generation""",2024-11-18 14:22:06.324261,False
http://arxiv.org/abs/2409.02864v2,Language Model Powered Digital Biology,"  Recent advancements in Large Language Models (LLMs) are transforming biology,
computer science, and many other research fields, as well as impacting everyday
life. While transformer-based technologies are currently being deployed in
biology, no available agentic system has been developed to tackle
bioinformatics workflows. We present a prototype Bioinformatics Retrieval
Augmented Data (BRAD) digital assistant. BRAD is a chatbot and agentic system
that integrates a suite of tools to handle bioinformatics tasks, from code
execution to online search. We demonstrate its capabilities through (1)
improved question-and-answering with retrieval augmented generation (RAG), (2)
the ability to run complex software pipelines, and (3) the ability to organize
and distribute tasks in agentic workflows. We use BRAD for automation,
performing tasks ranging from gene enrichment and searching the archive to
automatic code generation for running biomarker identification pipelines. BRAD
is a step toward autonomous, self-driving labs for digital biology.
",0.8305579874684431,"RAG AND ""code generation""",2024-11-18 14:22:06.835745,False
http://arxiv.org/abs/2401.01701v3,"De-Hallucinator: Mitigating LLM Hallucinations in Code Generation Tasks
  via Iterative Grounding","  Large language models (LLMs) trained on datasets of publicly available source
code have established a new state of the art in code generation tasks. However,
these models are mostly unaware of the code that exists within a specific
project, preventing the models from making good use of existing APIs. Instead,
LLMs often invent, or ""hallucinate"", non-existent APIs or produce variants of
already existing code. This paper presents De-Hallucinator, a technique that
grounds the predictions of an LLM through a novel combination of retrieving
suitable API references and iteratively querying the model with increasingly
suitable context information in the prompt. The approach exploits the
observation that predictions by LLMs often resemble the desired code, but they
fail to correctly refer to already existing APIs. De-Hallucinator automatically
identifies project-specific API references related to the model's initial
predictions and adds these references into the prompt. Unlike
retrieval-augmented generation (RAG), our approach uses the initial
prediction(s) by the model to iteratively retrieve increasingly suitable API
references. Our evaluation applies the approach to two tasks: predicting API
usages in Python and generating tests in JavaScript. We show that
De-Hallucinator consistently improves the generated code across five LLMs. In
particular, the approach improves the edit distance by 23.3-50.6% and the
recall of correctly predicted API usages by 23.9-61.0% for code completion, and
improves the number of fixed tests that initially failed because of
hallucinations by 63.2%, resulting in a 15.5% increase in statement coverage
for test generation.
",0.8264778660788137,"RAG AND ""code generation""",2024-11-18 14:22:07.427230,False
http://arxiv.org/abs/2405.13057v1,Can Github issues be solved with Tree Of Thoughts?,"  While there have been extensive studies in code generation by large language
models (LLM), where benchmarks like HumanEval have been surpassed with an
impressive 96.3% success rate, these benchmarks predominantly judge a model's
performance on basic function-level code generation and lack the critical
thinking and concept of scope required of real-world scenarios such as solving
GitHub issues. This research introduces the application of the Tree of Thoughts
(ToT) language model reasoning framework for enhancing the decision-making and
problem-solving abilities of LLMs for this complex task. Compared to
traditional input-output (IO) prompting and Retrieval Augmented Generation
(RAG) techniques, ToT is designed to improve performance by facilitating a
structured exploration of multiple reasoning trajectories and enabling
self-assessment of potential solutions. We experimentally deploy ToT in
tackling a Github issue contained within an instance of the SWE-bench. However,
our results reveal that the ToT framework alone is not enough to give LLMs the
critical reasoning capabilities to outperform existing methods. In this paper
we analyze the potential causes of these shortcomings and identify key areas
for improvement such as deepening the thought process and introducing agentic
capabilities. The insights of this research are aimed at informing future
directions for refining the application of ToT and better harnessing the
potential of LLMs in real-world problem-solving scenarios.
",0.8255373058928199,"RAG AND ""code generation""",2024-11-18 14:22:07.935409,False
http://arxiv.org/abs/2407.03889v1,"Automated C/C++ Program Repair for High-Level Synthesis via Large
  Language Models","  In High-Level Synthesis (HLS), converting a regular C/C++ program into its
HLS-compatible counterpart (HLS-C) still requires tremendous manual effort.
Various program scripts have been introduced to automate this process. But the
resulting codes usually contain many issues that should be manually repaired by
developers. Since Large Language Models (LLMs) have the ability to automate
code generation, they can also be used for automated program repair in HLS.
However, due to the limited training of LLMs considering hardware and software
simultaneously, hallucinations may occur during program repair using LLMs,
leading to compilation failures. Besides, using LLMs for iterative repair also
incurs a high cost. To address these challenges, we propose an LLM-driven
program repair framework that takes regular C/C++ code as input and
automatically generates its corresponding HLS-C code for synthesis while
minimizing human repair effort. To mitigate the hallucinations in LLMs and
enhance the prompt quality, a Retrieval-Augmented Generation (RAG) paradigm is
introduced to guide the LLMs toward correct repair. In addition, we use LLMs to
create a static bit width optimization program to identify the optimized bit
widths for variables. Moreover, LLM-driven HLS optimization strategies are
introduced to add/tune pragmas in HLS-C programs for circuit optimization.
Experimental results demonstrate that the proposed LLM-driven automated
framework can achieve much higher repair pass rates in 24 real-world
applications compared with the traditional scripts and the direct application
of LLMs for program repair.
",0.8054396771991228,"RAG AND ""code generation""",2024-11-18 14:22:08.513523,False
http://arxiv.org/abs/2410.18792v2,An LLM Agent for Automatic Geospatial Data Analysis,"  Large language models (LLMs) are being used in data science code generation
tasks, but they often struggle with complex sequential tasks, leading to
logical errors. Their application to geospatial data processing is particularly
challenging due to difficulties in incorporating complex data structures and
spatial constraints, effectively utilizing diverse function calls, and the
tendency to hallucinate less-used geospatial libraries. To tackle these
problems, we introduce GeoAgent, a new interactive framework designed to help
LLMs handle geospatial data processing more effectively. GeoAgent pioneers the
integration of a code interpreter, static analysis, and Retrieval-Augmented
Generation (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm,
offering a novel approach to geospatial data processing. In addition, we
contribute a new benchmark specifically designed to evaluate the LLM-based
approach in geospatial tasks. This benchmark leverages a variety of Python
libraries and includes both single-turn and multi-turn tasks such as data
acquisition, data analysis, and visualization. By offering a comprehensive
evaluation among diverse geospatial contexts, this benchmark sets a new
standard for developing LLM-based approaches in geospatial data analysis tasks.
Our findings suggest that relying solely on knowledge of LLM is insufficient
for accurate geospatial task programming, which requires coherent multi-step
processes and multiple function calls. Compared to the baseline LLMs, the
proposed GeoAgent has demonstrated superior performance, yielding notable
improvements in function calls and task completion. In addition, these results
offer valuable insights for the future development of LLM agents in automatic
geospatial data analysis task programming.
",0.8123083830395522,"RAG AND ""code generation""",2024-11-18 14:22:09.638825,False
