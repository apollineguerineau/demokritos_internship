{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_papers(labelled_data, fetched_pages) : \n",
    "    left_join = pd.merge(\n",
    "        fetched_pages,\n",
    "        labelled_data,\n",
    "        on=\"url\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_labelled\"), \n",
    "        indicator=True,\n",
    "    )\n",
    "    columns_to_keep = [col for col in left_join.columns if not col.endswith(\"_labelled\")]\n",
    "    left_join = left_join[columns_to_keep]\n",
    "    only_in_fetched = left_join[left_join[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])\n",
    "    return(only_in_fetched)\n",
    "\n",
    "def get_already_labelled_papers(original_dataset, fetched_pages):\n",
    "    left_join = pd.merge(\n",
    "        fetched_pages,\n",
    "        original_dataset,\n",
    "        on=\"url\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_labelled\"),\n",
    "        indicator=True,\n",
    "    )\n",
    "    columns_to_keep = [col for col in left_join.columns if not col.endswith(\"_labelled\")]\n",
    "    left_join = left_join[columns_to_keep]\n",
    "    already_labelled = left_join[left_join[\"_merge\"] == \"both\"].drop(columns=[\"_merge\"])\n",
    "    already_labelled.drop(columns=[\"relevance\"], inplace=True)\n",
    "    return already_labelled\n",
    "\n",
    "def get_topk_threshold(fetched_pages, top_k):\n",
    "    sorted_pages = fetched_pages.sort_values(by='score', ascending=False)\n",
    "    if top_k > len(sorted_pages):\n",
    "        topk_score = sorted_pages.iloc[-1]['score']\n",
    "    else : \n",
    "        topk_score = sorted_pages.iloc[top_k - 1]['score']\n",
    "    return(topk_score)\n",
    "\n",
    "def keep_above_threshold(thresholds,fetched_pages):\n",
    "    kept = fetched_pages[fetched_pages['score'] >= thresholds]\n",
    "    return(kept)\n",
    "\n",
    "def remove_duplicates(lst):\n",
    "    seen = set()\n",
    "    return [x for x in lst if not (x in seen or seen.add(x))]\n",
    "\n",
    "def sep_by_query(session_infos, fetched_pages):\n",
    "    papers_by_query = {}\n",
    "    queries = remove_duplicates(list(session_infos['all_queries'])[0].split(';'))\n",
    "    for query in queries:\n",
    "        papers = fetched_pages[fetched_pages['get_with_query'] == query]\n",
    "        papers_by_query[query]=papers\n",
    "    return(papers_by_query)\n",
    "\n",
    "def get_cumulative_papers(query_dict):\n",
    "    cumulative_papers = {}\n",
    "    seen_papers = pd.DataFrame()  # DataFrame initial pour cumuler les papiers\n",
    "    for query, papers in query_dict.items():\n",
    "        # Ajouter les nouveaux papiers\n",
    "        seen_papers = pd.concat([seen_papers, papers]).drop_duplicates().reset_index(drop=True)\n",
    "        # CrÃ©er un DataFrame pour la query en cours avec les colonnes des DataFrames originaux\n",
    "        cumulative_papers[query] = seen_papers.copy()\n",
    "    return cumulative_papers\n",
    "\n",
    "def keep_topk_by_query(session_infos, fetched_pages, top_k):\n",
    "    papers_by_query = sep_by_query(session_infos, fetched_pages)\n",
    "    cumulative_papers_by_query = get_cumulative_papers(papers_by_query)\n",
    "    kept_papers_by_query = {}\n",
    "    for query, papers in cumulative_papers_by_query.items() : \n",
    "        threshold = get_topk_threshold(papers, top_k)\n",
    "        kept_papers = keep_above_threshold(threshold, papers)\n",
    "        kept_papers_by_query[query]=kept_papers\n",
    "    return(kept_papers_by_query)\n",
    "\n",
    "def recall_precision(fetched_pages, labelled_dataset) : \n",
    "    labelled_data_relevants = labelled_dataset[labelled_dataset['relevance']==1]\n",
    "    merged_data = pd.merge(labelled_dataset, fetched_pages, on=\"url\", how=\"inner\")\n",
    "    merged_data['relevance'] = merged_data['relevance'].fillna(0)\n",
    "    relevants = merged_data[merged_data['relevance']==1]\n",
    "    recall = len(relevants) / len(labelled_data_relevants) if len(labelled_data_relevants) > 0 else 0\n",
    "    precision = len(relevants) / len(merged_data) if len(merged_data) > 0 else 0\n",
    "    return(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_query = 'RAG_AND_\"code_generation\"/'\n",
    "# initial_query = '\"Machine_Learning\"_AND_(diffusion_OR_diffusivity)_AND_(MOFs_OR_ZIFs_OR_\"metal-organic_frameworks\"_OR_COFs_OR_\"covalent-organic_frameworks)/'\n",
    "initial_query = '\"metal-organic_frameworks\"_AND_\"material_design\"_AND_\"properties\"/'\n",
    "\n",
    "base_path = f'/Users/apollineguerineau/Documents/ENSAI/3A/Greece/internship/eval/results/{initial_query}'\n",
    "original_dataset = pd.read_csv(base_path +'original_dataset.csv', sep=';')\n",
    "labelled_new_papers_path = \"/Users/apollineguerineau/Documents/ENSAI/3A/Greece/internship/eval/ML_MOF_Diffusion/crawlers_v1/new_papers_to_label.csv\"\n",
    "path_baseline = base_path + 'baseline/'\n",
    "path_seed_query_expand = base_path + 'SeedQueryBasedTemplate__/'\n",
    "path_most_relevant_expand = base_path + 'MostRelevantPagesBasedTemplate_MostRelevantPagesPromptBasedTemplate_HydeBasedTemplate/'\n",
    "crawlers = {'baseline':path_baseline,\n",
    "            'seed_query_expand_sim_cos':path_seed_query_expand,\n",
    "            # 'seed_query_expand_sim_cos':path_seed_query_expand+'sim_cos/',\n",
    "            # 'seed_query_expand_hyde_sim_cos':path_seed_query_expand+'hyde_sim_cos/',\n",
    "            'most_relevant_expand':path_most_relevant_expand}\n",
    "\n",
    "top_k = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reform_sessions_infos(crawler_path):\n",
    "    df = pd.read_csv(crawler_path +'session_infos.csv')\n",
    "    session_name = str(df['session_name'].iloc[0])\n",
    "    searcher = str(df['searcher'].iloc[0])\n",
    "    query_expansion = str(df['query_expansion'].iloc[0])\n",
    "    classifier = str(df['classifier'].iloc[0])\n",
    "    queries = str(df['nb_pages_per_request'].iloc[0])\n",
    "    nb_seed_pages = str(df['stop_criteria'].iloc[0])\n",
    "    duration = str(df['all_queries'].iloc[0])\n",
    "    hyde = str(df['threshold'].iloc[0])\n",
    "    nb_fetched_pages = str(df['hyde'].iloc[0])\n",
    "    cols = ['session_name', 'searcher', 'query_expansion', 'classifier', 'all_queries', 'nb_seed_pages', 'duration', 'hyde', 'nb_fetched_pages']\n",
    "    line = [session_name, searcher, query_expansion, classifier, queries, nb_seed_pages, duration, hyde, nb_fetched_pages]\n",
    "    new_df = pd.DataFrame([line], columns=cols)\n",
    "    return(new_df)\n",
    "\n",
    "for crawler, path in crawlers.items() : \n",
    "    df = reform_sessions_infos(path)\n",
    "    df.to_csv(path+'session.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most_relevant_expand\n",
      "39 papers to label\n"
     ]
    }
   ],
   "source": [
    "## get all new fetched papers \n",
    "\n",
    "all_new_papers = []\n",
    "for crawler, path in crawlers.items() : \n",
    "    print(crawler)\n",
    "    fetched_pages = pd.read_csv(path + 'fetched_pages.csv')\n",
    "    session_infos = pd.read_csv(path + 'session.csv')\n",
    "    if crawler == 'baseline':\n",
    "        all_kept_papers = fetched_pages.iloc[:100]\n",
    "    else : \n",
    "        kept_papers_by_query = keep_topk_by_query(session_infos, fetched_pages, top_k)\n",
    "        all_kept_papers = pd.concat(kept_papers_by_query.values(), ignore_index=True)\n",
    "    new_papers = get_new_papers(original_dataset, all_kept_papers)\n",
    "    all_new_papers.append(new_papers)\n",
    "\n",
    "combined_new_papers = pd.concat(all_new_papers, ignore_index=True)\n",
    "combined_new_papers = combined_new_papers.drop_duplicates(subset=[\"url\"], keep=\"first\")\n",
    "\n",
    "if os.path.exists(labelled_new_papers_path):\n",
    "    labelled_new_papers = pd.read_csv(labelled_new_papers_path)  \n",
    "    combined_new_papers = pd.concat([combined_new_papers,labelled_new_papers], ignore_index=True)\n",
    "    combined_new_papers[\"has_relevance\"] = combined_new_papers[\"relevance\"].notna()\n",
    "    combined_new_papers = combined_new_papers.sort_values(by=\"has_relevance\", ascending=False)\n",
    "    combined_new_papers = combined_new_papers.drop_duplicates(subset=[\"url\"])\n",
    "    \n",
    "columns_to_keep = [\"url\", \"title\", \"description\", \"relevance\"]\n",
    "combined_new_papers = combined_new_papers.loc[:, columns_to_keep]\n",
    "combined_new_papers.to_csv(base_path + \"new_papers_to_label.csv\", index=False)\n",
    "\n",
    "missing_relevance = combined_new_papers[combined_new_papers['relevance'].isna()]\n",
    "print(f'{len(missing_relevance)} papers to label')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## recall and precision evolution by query\n",
    "\n",
    "labelled_new_papers = pd.read_csv(labelled_new_papers_path)\n",
    "labelled_dataset = pd.concat([labelled_new_papers,original_dataset], ignore_index=True)\n",
    "for crawler, path in crawlers.values() : \n",
    "    print(crawler)\n",
    "    fetched_pages = pd.read_csv(path + 'fetched_pages.csv')\n",
    "    session_infos = pd.read_csv(path + 'session.csv')\n",
    "    if crawler == 'baseline':\n",
    "        query = list(session_infos['all_queries'])[0]\n",
    "        kept_papers_by_query = {query:fetched_pages.iloc[:100]}\n",
    "    else : \n",
    "        kept_papers_by_query = keep_topk_by_query(session_infos, fetched_pages, top_k)\n",
    "    for query, papers in kept_papers_by_query : \n",
    "        print(f'query : {query} : {recall_precision(papers, labelled_dataset)}')\n",
    "    print('----------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
